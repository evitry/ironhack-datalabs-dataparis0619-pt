{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IronhackSpider:\n",
    "    \"\"\"\n",
    "    This is the constructor class to which you can pass a bunch of parameters. \n",
    "    These parameters are stored to the class instance variables so that the\n",
    "    class functions can access them later.\n",
    "    \n",
    "    url_pattern: the regex pattern of the web urls to scape\n",
    "    pages_to_scrape: how many pages to scrape\n",
    "    sleep_interval: the time interval in seconds to delay between requests. If <0, requests will not be delayed.\n",
    "    content_parser: a function reference that will extract the intended info from the scraped content.\n",
    "    \"\"\"\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    def scrape_url(self, url):\n",
    "        print(url)\n",
    "              \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Timeout\")\n",
    "            return None\n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(\"Too Many Redirects\")\n",
    "            return None\n",
    "        except requests.exceptions.SSLError:\n",
    "            print(\"SSL error\")\n",
    "            return None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(\"RequestException\", e)\n",
    "            return None\n",
    "\n",
    "        if response.status_code<300:\n",
    "            result = self.content_parser(response.content)\n",
    "        elif response.status_code>=400 and response.status_code<500:\n",
    "            print('request failed because the ressource either does not exist or is forbidden')\n",
    "            return None\n",
    "        else:\n",
    "            print('request failed because the response server encountered an error')\n",
    "            return None\n",
    "        self.output_results(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. I export the results into a csv file.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        my_df = pd.Series(r)\n",
    "        my_df.to_csv('webscraping_project.csv', sep=',', index=False, header=False, mode = 'a')\n",
    "        #mode='a' is equivalent to append for a list. Hence the results are not overwritten. \n",
    "\n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    \"\"\"In IronhackSpider.kickstart(), implement sleep_interval. \n",
    "    You will check if self.sleep_interval is larger than 0. \n",
    "    If so, tell the FOR loop to sleep the given amount of time before making the next request.\"\"\"\n",
    "    def kickstart(self):\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            if self.sleep_interval>0:\n",
    "                time.sleep(self.sleep_interval)\n",
    "            self.scrape_url(self.url_pattern % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://montessouricettes.fr/blog-montessori-ief/page/1/\n",
      "https://montessouricettes.fr/blog-montessori-ief/page/2/\n",
      "https://montessouricettes.fr/blog-montessori-ief/page/3/\n",
      "https://montessouricettes.fr/blog-montessori-ief/page/4/\n"
     ]
    }
   ],
   "source": [
    "URL_PATTERN = 'https://montessouricettes.fr/blog-montessori-ief/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = 4 # how many webpages to scrapge\n",
    "\n",
    "\"\"\"\n",
    "This is a custom parser function you will complete in the challenge.\n",
    "This function extracts the quotes.\n",
    "This function will be passed to the IronhackSpider class.\n",
    "\"\"\"\n",
    "def quotes_parser(content):\n",
    "    titles=[]\n",
    "    soup = BeautifulSoup(content, \"lxml\")\n",
    "    h2_tags = soup.find('div',{'class':'col-sm-8'}).find_all('h2',{'class':'post-title entry-title'})\n",
    "    for h2_tag in h2_tags:\n",
    "        a_tag=h2_tag.find('a')\n",
    "        text=a_tag.text\n",
    "        titles.append(text)\n",
    "    return titles\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
